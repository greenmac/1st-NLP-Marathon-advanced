{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128, 64)\n",
    "        self.layer3 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0767, 0.1559, 0.0704, 0.0837, 0.0898, 0.1215, 0.1144, 0.0999, 0.0863,\n",
      "         0.1013],\n",
      "        [0.0882, 0.0661, 0.0965, 0.1918, 0.0649, 0.1566, 0.0680, 0.0966, 0.0781,\n",
      "         0.0933],\n",
      "        [0.0951, 0.1426, 0.0670, 0.1848, 0.0609, 0.1000, 0.1120, 0.0876, 0.0503,\n",
      "         0.0996],\n",
      "        [0.1500, 0.0733, 0.1128, 0.1429, 0.0537, 0.0859, 0.1112, 0.0902, 0.1083,\n",
      "         0.0716]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同\n",
    "* 如果欲使用LogSoftmax, CrossEntropyLoss，可以將 nn.Softmax從模型中移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NLLLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch.log(output), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0446,  0.0082, -0.0173,  ...,  0.0399,  0.0332, -0.0362],\n",
      "        [-0.0303,  0.0310,  0.0191,  ..., -0.0477, -0.0176, -0.0123],\n",
      "        [-0.0099,  0.0209,  0.0314,  ..., -0.0114,  0.0314,  0.0276],\n",
      "        ...,\n",
      "        [ 0.0527, -0.0160, -0.0030,  ..., -0.0336, -0.0623, -0.0210],\n",
      "        [-0.0526,  0.0045,  0.0158,  ...,  0.0139,  0.0176,  0.0476],\n",
      "        [-0.0596, -0.0317, -0.0177,  ...,  0.0103, -0.0516, -0.0594]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0105, -0.0091,  0.0691,  ...,  0.0725,  0.0131,  0.0019],\n",
      "        [ 0.0080,  0.0071,  0.0099,  ..., -0.0480, -0.0463, -0.0009],\n",
      "        [ 0.0055,  0.0057, -0.0261,  ..., -0.0230,  0.0064, -0.0041],\n",
      "        ...,\n",
      "        [ 0.0356,  0.0198, -0.0116,  ..., -0.0248,  0.0284, -0.0326],\n",
      "        [-0.0024, -0.0012,  0.0017,  ...,  0.0013, -0.0027,  0.0022],\n",
      "        [-0.0021, -0.0122,  0.0096,  ...,  0.0365,  0.0034,  0.0033]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0436,  0.0092, -0.0183,  ...,  0.0389,  0.0322, -0.0372],\n",
      "        [-0.0313,  0.0300,  0.0181,  ..., -0.0467, -0.0166, -0.0113],\n",
      "        [-0.0109,  0.0199,  0.0324,  ..., -0.0104,  0.0304,  0.0286],\n",
      "        ...,\n",
      "        [ 0.0517, -0.0170, -0.0020,  ..., -0.0326, -0.0633, -0.0200],\n",
      "        [-0.0516,  0.0055,  0.0148,  ...,  0.0129,  0.0186,  0.0466],\n",
      "        [-0.0586, -0.0307, -0.0187,  ...,  0.0093, -0.0526, -0.0604]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0105, -0.0091,  0.0691,  ...,  0.0725,  0.0131,  0.0019],\n",
      "        [ 0.0080,  0.0071,  0.0099,  ..., -0.0480, -0.0463, -0.0009],\n",
      "        [ 0.0055,  0.0057, -0.0261,  ..., -0.0230,  0.0064, -0.0041],\n",
      "        ...,\n",
      "        [ 0.0356,  0.0198, -0.0116,  ..., -0.0248,  0.0284, -0.0326],\n",
      "        [-0.0024, -0.0012,  0.0017,  ...,  0.0013, -0.0027,  0.0022],\n",
      "        [-0.0021, -0.0122,  0.0096,  ...,  0.0365,  0.0034,  0.0033]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0436,  0.0092, -0.0183,  ...,  0.0389,  0.0322, -0.0372],\n",
      "        [-0.0313,  0.0300,  0.0181,  ..., -0.0467, -0.0166, -0.0113],\n",
      "        [-0.0109,  0.0199,  0.0324,  ..., -0.0104,  0.0304,  0.0286],\n",
      "        ...,\n",
      "        [ 0.0517, -0.0170, -0.0020,  ..., -0.0326, -0.0633, -0.0200],\n",
      "        [-0.0516,  0.0055,  0.0148,  ...,  0.0129,  0.0186,  0.0466],\n",
      "        [-0.0586, -0.0307, -0.0187,  ...,  0.0093, -0.0526, -0.0604]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : None\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
