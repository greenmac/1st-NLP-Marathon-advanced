{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 89527\n",
      "vocab length after removing stopwords: 89356\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "with open('../data/big/aclImdb/imdb.vocab') as f:\n",
    "    vocab = f.read()\n",
    "vocab = vocab.split('\\n')\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "vocab = list(set(vocab).difference(set(stopwords.words('english'))))\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "vocab_dict = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('../data/big/aclImdb/train/pos/4715_9.txt', 1), ('../data/big/aclImdb/train/pos/12390_8.txt', 1)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "\n",
    "review_pos = glob.glob(\"../data/big/aclImdb/train/pos/*.txt\")\n",
    "review_neg = glob.glob(\"../data/big/aclImdb/train/neg/*.txt\")\n",
    "review_all = review_pos + review_neg\n",
    "y = [1]*len(review_pos) + [0]*len(review_neg)\n",
    "\n",
    "review_pairs = list(zip(review_all, y))\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    \n",
    "    with open(review_path, 'r') as f:\n",
    "        review = f.read()\n",
    "        \n",
    "    #移除non-alphabet符號、贅字與tokenize\n",
    "    review = re.sub('[^a-zA-Z]',' ',review)\n",
    "    review = nltk.word_tokenize(review)\n",
    "    review = list(set(review).difference(set(stopwords.words('english'))))\n",
    "    \n",
    "    return review\n",
    "\n",
    "def generate_vec(review, vocab_dic):\n",
    "    doc_vec = []\n",
    "    for word in review:\n",
    "        if vocab_dic.get(word):\n",
    "            doc_vec.append(vocab_dic.get(word))\n",
    "            \n",
    "    return torch.tensor(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    def __init__(self, data_dirs, vocab):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data_dirs[idx]\n",
    "        review = pair[0]\n",
    "        review = load_review(review)\n",
    "        review = generate_vec(review, self.vocab)\n",
    "        \n",
    "        return review, pair[1]\n",
    "    \n",
    "\n",
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "\n",
    "    corpus, labels = zip(*batch) \n",
    "    \n",
    "    ### create pads for corpus ###\n",
    "    lengths = [len(x) for x in corpus]\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    batch_corpus = []\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        # pad corpus\n",
    "        tmp_pads = torch.zeros(max_length)\n",
    "        tmp_pads[:lengths[i]] = corpus[i]\n",
    "        tmp_pads.view(-1, 1)\n",
    "        batch_corpus.append(tmp_pads.view(1,-1))\n",
    "\n",
    "    return torch.cat(batch_corpus,dim=0), torch.tensor(labels) , torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[77349., 58126., 41094., 85660., 46369., 74876., 56742., 80012., 45500.,\n",
       "          70873., 41193., 58486.,  6520.,  1138.,  3539., 79402., 87572., 58612.,\n",
       "           1101., 42493., 85968., 81538., 61612., 54878., 16516., 80977., 13119.,\n",
       "          80180., 35996., 24291., 39679., 69950., 68051., 42322., 86779., 53552.,\n",
       "          45559., 70735., 76619., 22676.,  5433., 22999.,  6325., 33797.,  6600.,\n",
       "          45081., 21504., 27036., 39729.,  4256., 83140., 37767., 65967.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.],\n",
       "         [65185., 32654., 85277., 37937., 56314., 70108., 79623., 34285., 82983.,\n",
       "          29446., 40855.,  6657.,  9161., 62578., 78260., 29997., 42493., 86807.,\n",
       "           8087., 16346.,  2124., 53739., 10854.,  1793., 16516., 73714.,  5056.,\n",
       "           6669., 49339., 13559., 21083., 66302., 35701.,  5433., 55816., 33047.,\n",
       "           8655., 68288.,  6325., 74268., 42876.,  8846., 71477., 33801.,  5794.,\n",
       "          22119., 83717., 23691., 37112., 76023., 45094., 41059.,  8115.,  8495.,\n",
       "          34489., 20773., 81061., 54269., 21954.,  8862., 57403., 30381., 85339.,\n",
       "          30716., 82647., 17672., 55846., 46519., 19736.,  8687.,  4064., 41248.,\n",
       "          15410., 38373., 16583., 46026.,  7084., 16765., 10084., 24972., 24419.,\n",
       "          32561., 39061., 27391., 50464., 78503., 69258.,  5663., 30914., 16264.,\n",
       "           6745., 48188.,  1138., 43250., 72215., 42035.,  6933., 37181., 61646.,\n",
       "          48380., 46056., 80594., 35429., 52942., 74846., 77836., 74682., 79221.,\n",
       "           8731., 19593.,  8191.,  9960., 65471., 51346., 46562., 66925., 88239.,\n",
       "          27596., 50840., 61011., 30604., 25712., 13269., 29206., 31944., 59112.,\n",
       "          74363., 44814., 51736.,  1726., 28701., 46417., 57836., 63752., 77132.,\n",
       "          62366., 59124., 59123., 35645., 57140., 87572., 77697., 48247., 13119.,\n",
       "          75948., 18796., 49641., 74904., 24331., 75573., 78941., 18641., 51219.,\n",
       "          40676., 60181., 43326.,  7345.,  6116., 26101., 87108.,  5199., 10326.,\n",
       "          61709.,  2255., 55076., 16323.],\n",
       "         [26612.,  6375., 72835., 66054., 45269., 41538., 29907., 30829., 88504.,\n",
       "           6976., 61903., 70639., 82323., 20968., 47440., 56662., 24032., 81438.,\n",
       "          16698., 78260., 69992., 26202., 16516., 10946., 30716., 13117., 35613.,\n",
       "          55427., 29927., 84763., 84271.,  5056., 76523., 33832., 20950.,  2656.,\n",
       "          59179., 88915., 11170., 71287.,  7084., 25871., 78373., 65899., 68976.,\n",
       "          25248., 78279., 69394., 14705., 49909.,  7131., 45081.,  5199., 34726.,\n",
       "          12013., 49532.,  6838., 32649., 61806., 74408.,  8432., 15766.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.],\n",
       "         [46287., 87305., 15507., 40536., 56314., 70108., 16337., 18324., 13868.,\n",
       "          34795., 22276., 33579., 59516., 64308., 42493., 28375., 35881., 16516.,\n",
       "          39005., 41565.,  5056., 77006., 52689.,  6669., 69950., 59009., 53896.,\n",
       "          37459., 20747.,  5071., 20251., 33801., 80691., 45613., 23691., 69210.,\n",
       "          85660.,  3869., 72535., 19211., 28416., 70970., 34161., 33255.,  1101.,\n",
       "          10070., 63016., 35728., 80180., 84803., 55846., 26682., 16239., 78313.,\n",
       "          12290., 31524., 69434., 48008., 57952., 30909., 11420., 49219., 61985.,\n",
       "          71002.,  2174., 50107., 77640.,  6223.,  3573., 67620., 65646., 80594.,\n",
       "          86204., 35429., 37369., 46556., 70842., 44977., 19593., 54501., 40134.,\n",
       "          51346., 56249.,   643., 51905., 24293.,  2736., 83809., 58002., 80440.,\n",
       "          54174., 52799.,  6976.,  1373., 60339., 19472., 33529.,  7501., 21364.,\n",
       "          69687., 48247., 41698., 57510., 68393., 53001., 40676., 87761., 12586.,\n",
       "          26101., 57864., 77890., 65352., 79599., 47591., 17393., 31461., 83660.,\n",
       "           7362.,  2255., 23334.,  8432., 68979.,  2104.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.]]),\n",
       " tensor([0, 0, 0, 1]),\n",
       " tensor([ 53, 166,  62, 123]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "custom_dst = dataset(review_pairs, vocab_dict)\n",
    "custom_dataloader = DataLoader(dataset=custom_dst, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "next(iter(custom_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa3dd84ef0f650d9d8b867844db5a915f08cb58b5d6dab20fc53865ee0283ae4"
  },
  "kernelspec": {
   "display_name": "cupoy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
