{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128, 64)\n",
    "        self.layer3 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/8lfclbhs4bj58zsxyz2zmb140000gn/T/ipykernel_39001/1550778189.py:6: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0873, 0.1630, 0.1030, 0.0581, 0.0688, 0.0928, 0.1172, 0.0794, 0.0998,\n",
      "         0.1304],\n",
      "        [0.0745, 0.1713, 0.1188, 0.0960, 0.0664, 0.1576, 0.0818, 0.0686, 0.0605,\n",
      "         0.1045],\n",
      "        [0.0921, 0.1022, 0.0569, 0.0800, 0.0331, 0.1429, 0.1450, 0.0895, 0.2009,\n",
      "         0.0573],\n",
      "        [0.1026, 0.1387, 0.0787, 0.0899, 0.0652, 0.0949, 0.1724, 0.0877, 0.0683,\n",
      "         0.1017]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(torch.log(output), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0117, -0.0499,  0.0005,  ..., -0.0091, -0.0426,  0.0455],\n",
      "        [ 0.0399,  0.0129, -0.0463,  ...,  0.0577, -0.0294, -0.0001],\n",
      "        [-0.0487,  0.0520, -0.0123,  ..., -0.0055, -0.0191, -0.0345],\n",
      "        ...,\n",
      "        [-0.0213, -0.0605, -0.0242,  ...,  0.0586, -0.0527, -0.0565],\n",
      "        [ 0.0481, -0.0156,  0.0054,  ..., -0.0406,  0.0202,  0.0330],\n",
      "        [ 0.0326, -0.0622,  0.0541,  ..., -0.0419, -0.0377,  0.0122]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ 1.9297e-03, -1.6583e-03, -1.1734e-03,  ...,  5.9098e-03,\n",
      "         -1.8262e-03,  4.4991e-03],\n",
      "        [-3.6286e-04, -2.6425e-04,  8.4127e-05,  ..., -1.8569e-04,\n",
      "         -3.8107e-04, -3.9645e-05],\n",
      "        [ 4.2613e-02,  1.7764e-01,  3.1588e-03,  ..., -1.0676e-01,\n",
      "          1.3270e-01, -9.7836e-02],\n",
      "        ...,\n",
      "        [ 5.7389e-02,  4.1178e-02,  3.0519e-03,  ..., -3.5413e-02,\n",
      "          1.1378e-01, -5.6627e-02],\n",
      "        [-4.2246e-02, -3.7635e-02,  2.7219e-02,  ..., -2.3230e-02,\n",
      "         -6.7615e-02, -1.0354e-02],\n",
      "        [ 5.1728e-05,  2.5586e-04, -2.5996e-05,  ..., -6.1707e-05,\n",
      "          1.1193e-04, -4.5415e-05]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0127, -0.0489,  0.0015,  ..., -0.0101, -0.0416,  0.0445],\n",
      "        [ 0.0409,  0.0139, -0.0473,  ...,  0.0587, -0.0284,  0.0009],\n",
      "        [-0.0497,  0.0510, -0.0133,  ..., -0.0045, -0.0201, -0.0335],\n",
      "        ...,\n",
      "        [-0.0223, -0.0615, -0.0252,  ...,  0.0596, -0.0537, -0.0555],\n",
      "        [ 0.0491, -0.0146,  0.0044,  ..., -0.0396,  0.0212,  0.0340],\n",
      "        [ 0.0316, -0.0632,  0.0531,  ..., -0.0409, -0.0387,  0.0132]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ 1.9297e-03, -1.6583e-03, -1.1734e-03,  ...,  5.9098e-03,\n",
      "         -1.8262e-03,  4.4991e-03],\n",
      "        [-3.6286e-04, -2.6425e-04,  8.4127e-05,  ..., -1.8569e-04,\n",
      "         -3.8107e-04, -3.9645e-05],\n",
      "        [ 4.2613e-02,  1.7764e-01,  3.1588e-03,  ..., -1.0676e-01,\n",
      "          1.3270e-01, -9.7836e-02],\n",
      "        ...,\n",
      "        [ 5.7389e-02,  4.1178e-02,  3.0519e-03,  ..., -3.5413e-02,\n",
      "          1.1378e-01, -5.6627e-02],\n",
      "        [-4.2246e-02, -3.7635e-02,  2.7219e-02,  ..., -2.3230e-02,\n",
      "         -6.7615e-02, -1.0354e-02],\n",
      "        [ 5.1728e-05,  2.5586e-04, -2.5996e-05,  ..., -6.1707e-05,\n",
      "          1.1193e-04, -4.5415e-05]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0127, -0.0489,  0.0015,  ..., -0.0101, -0.0416,  0.0445],\n",
      "        [ 0.0409,  0.0139, -0.0473,  ...,  0.0587, -0.0284,  0.0009],\n",
      "        [-0.0497,  0.0510, -0.0133,  ..., -0.0045, -0.0201, -0.0335],\n",
      "        ...,\n",
      "        [-0.0223, -0.0615, -0.0252,  ...,  0.0596, -0.0537, -0.0555],\n",
      "        [ 0.0491, -0.0146,  0.0044,  ..., -0.0396,  0.0212,  0.0340],\n",
      "        [ 0.0316, -0.0632,  0.0531,  ..., -0.0409, -0.0387,  0.0132]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
